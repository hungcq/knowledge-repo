# S3
- Use cases:
  - Backup & storage
  - Disaster recovery
  - Archive
  - Hybrid cloud storage
  - Host app
  - Host media
  - Data lakes & big data analytics
  - Software delivery
  - Static website: URL: `http://bucket-name.s3-website[. or -]Region.amazonaws.com`
- Buckets:
  - Store objects (files)
  - Must have globally unique name across all regions & accounts
  - Defined at region level
  - Naming convention: lower case letter/number/hyphen/dot, except some prefix/suffix (xn-, -s3alias), not an IP
- Objects:
  - Key:
    - Def: full path to an object
    - Contains: s3://bucket-name/prefix/prefix/object_name
    - -> No directories within bucket, just long name with /
  - Value:
    - Max size: 5TB
    - If upload more than 5GB, must use multipart upload
  - Metadata: system/user: list of text key-value pairs.
  - Tags: Unicode key-value pairs (max 10). Useful for security/lifecycle.
  - Version ID if versioning is enable
- Versioning:
  - Can be enabled at bucket level
  - Same key override will change version
  - Best practice to version bucket: recovery & version control
  - File exist before enabling versioning will have version null
  - Disabling versioning won't remove previous versions
- Replication:
  - Must enable versioning in source & destination buckets
  - Types:
    - Cross region replication (CRR)
    - Same region replication (SRR)
  - Async
  - Buckets can be in dif AWS accounts
  - S3 must have proper permissions (read & write)
  - Use cases:
    - CRR: compliance, low latency access, cross account replication
    - SRR: log aggregation, replication between production & test env
  - Notes:
    - Only new objects are replicated after enabling
    - For existing/failed objects, use batch replication
    - Can replicate delete markers, but not version ID
    - No direct chaining of replication: A -> B -> C, no A -> C
- 2 ways to copy large amount of existing data between buckets:
  - Use "aws S3 sync" command
  - Use batch replication
- -> Console can only be used for small numbers of objects
- Requester pays bucket:
  - Normally: owner pay for storage & data transfer cost
  - Requester pays for cost of request & data download
  - Use case: share large data set with other accounts
  - Requester must be authenticated in AWS
- Event notification:
  - Events: eg object created/removed
  - Async
  - Can filter by name (eg *.jpg)
  - Use case: generate thumbnails of uploaded images
  - Target: SNS, SQS (FIFO not supported), Lambda, EventBridge
  - Event noti via Event Bridge: flow: setup rules in Bridge -> S3 send all events to bridge -> Bridge can send events to 18 services
  - Need IAM resource access policy (on target) for S3 service to send event to target
- Performance:
  - Baseline:
    - Autoscale to high request rate, 100-200ms latency
    - Min 3500 write (PUT/COPY/POST/DELETE), 5500 read (GET/HEAD) requests/s/prefix/bucket
  - Optimization:
    - Multipart upload: recommended for files > 100MB, required for files > 5GB
    - -> Allow parallelization
    - Transfer acceleration: increase transfer speed by transferring to AWS edge location.
      Edge location will forward to target S3 bucket via private network.
    - Byte-range fetch: parallelize GET by requesting specific byte ranges. Use cases:
      - Better resilience in case of failure
      - Speed up download
      - Getting part of a file
- Select & Glacier Select: retrieve less data by using SQL to perform server-side filtering
- Batch operation:
  - Perform bulk operation on existing S3 objects with a single request
  - Use cases:
    - Modify object metadata/properties
    - Copy objects between buckets
    - Encrypt unencrypted objects (highlight)
    - Modify ACLs/tags
    - Restore objects from Glacier
    - Invoke Lambda function to do custom function on each object
  - Job components:
    - List objects
    - Action
    - Optional params
  - Features: manage retries, track progress, gen report, send noti...
  - Can use S3 Inventory to get object lists & Select to filter objects
## Storage classes & transition
- Set for each object within bucket
- Classes:
  - Standard - general purpose:
    - 99.99% availability
    - Used for frequently accessed data: low latency, high throughput
    - Sustain 2 concurrent facility failures
  - Infrequent access:
    - Used for less frequently accessed data, but requiring rapid access when needed
    - Lower cost compared to standard GP
    - Min storage duration: 30 days
    - Retrieval cost money
    - Types:
      - Standard:
        - 99.9% availability
        - Use cases: DR, backup
      - One-zone:
        - 99.5% availability. Data loss when AZ is destroyed.
        - Use cases: secondary copies, data that can be recreated
  - Glacier:
    - Low cost
    - Used for archiving/backup
    - Types:
      - Instant retrieval. Min storage duration: 90 days.
      - Flexible retrieval:
        - Types: expedited (1-5 mins), standard (3-5 hours), bulk (5-12 hours) - free
        - Min storage duration: 90 days
      - Deep archive:
        - For long term storage. Min storage duration: 180 days.
        - Types: standard (12 hours), bulk (48 hours)
  - Intelligent tiering:
    - Small monthly monitoring & auto-tiering fee
    - Move object automatically between access tiers based on usage
    - No retrieval charge
    - Types:
      - Frequent access: default
      - Infrequent access: > 30 days not accessed
      - Archive instant access: > 90 days
      - Archive access: configurable, > 90 - 700 days
      - Deep archive access: configurable, > 180 - 700 days
- -> Durability: same for all storage classes. Availability: varies depending on storage class.
- Move between storage classes:
  - Manually or via S3 lifecycle management
  - Can move to lower classes:
    standard > standard IA > intelligent tiering > one zone IA > glacier (instant > flexible > deep archive)
  - Automated using lifecycle rule:
    - Transition action: config object to move to another storage class
    - Expiration action: delete object/version/incomplete multipart upload after some time
  - -> Can be specified using prefix (with *) or tag
  - Storage Class Analysis:
    - Help decide when to transition objects to the right storage class
    - Recommendation for transition from Standard to Standard IA to create lifecycle rule. Not work for other classes.
    - Daily report
    - 24-48 hours to get initial report
## Security
- Access control:
  - User-based: IAM policies/roles: which user/instance can call which APIs
  - Resource-based:
    - Bucket policies:
      - Bucket wise rule from S3 console, applies to all accounts (cross-acc, public viewer)
      - JSON based. Attributes:
        - Version
        - Statement:
          - Resources: buckets/objects
          - Effect: allow/deny
          - Actions: set of API to allow/deny
          - Principal: account/user
    - Object access control list (ACL): finer grain (can be disabled)
    - Bucket access control list (ACL): less common (can be disabled)
- -> IAM principal can access an S3 object if: IAM permissions allows OR the resource policy allows AND no explicit deny
- Object encryption:
  - Server side:
    - S3 (AWS) managed keys:
      - Enabled by default
      - Key type: AES 256
      - Uploader can be force to add header via bucket policy. Header: "x-amz-server-side-encryption": "AES256".
      - Can't encrypt metadata
      - Each object encrypted with a unique key
    - KMS (key management service) keys:
      - User can control & audit key using CloudTrail
      - Uploader can be force to add header via bucket policy. Header: "x-amz-server-side-encryption": "aws:kms".
      - Limitation: KMS has limits (eg KMS API quota)
      - KMS API called when uploading: GenerateDataKey
      - KMS API called when downloading: Decrypt
    - Customer-provided keys:
      - HTTPs must be used
      - Key must be provided in HTTP headers
  - Client side:
    - Client use S3 Client Side Encryption lib
    - Client encrypt/decrypt data when sending/retrieving from S3
    - Client manages keys & encryption cycle
  - In transit (SSL/TLS):
    - 2 types of S3 endpoints: HTTP, HTTPS (encrypted)
    - Can be forced by bucket policy
- CORS: if client make CORS request on S3 bucket, need to enable CORS headers for specific or all (*) origins
- MFA delete:
  - Force users to use code on device to perform important S3 operations
  - Cases:
    - Permanently delete object versions
    - Disable bucket versioning
  - Conditions:
    - Versioning must be enabled
    - Only bucket owner/root account can enable/disable MFA delete
- Access logs:
  - Log all access to monitored buckets to a logging bucket (in same region)
  - -> Avoid using monitored buckets as logging bucket (logging loop)
  - Data can be analyzed using analysis tools
- Pre-signed URLs:
  - Generate using S3 console, AWS CLI/SDK
  - Expiration:
    - S3 console: 1 min - 12 hours
    - CLI: 1-168 hours
  - URL has GET/PUT permission of generating user
  - Use cases:
    - Allow logged-in users to download files
    - Allow users to upload file to a specific location in bucket
- Glacier Vault Lock:
  - Adopt WORM (write once read many) model
  - Process: create vault lock policy -> lock policy for future edits (can't change/delete)
  - Use case: compliance, data retention
- Object Lock:
  - Must enable versioning
  - Adopt WORM
  - Block an object version deletion for specific amount of time
  - Retention modes:
    - Compliance: ~Glacier Vault Lock:
      - No one can delete object versions
      - No one can change retention mode/period
    - Governance: privileged users can delete object versions/change retention mode/period
  - Legal hold:
    - Protect object indefinitely, independent of retention period
    - Can be added/removed with proper IAM permission
- Access Point:
  - With a policy attached
  - Have its own DNS name (public/private)
  - Goal: grant dif group of users with dif permissions
  - For VPC resources, need to connect to VPC endpoint (gateway/interface endpoint) to access the access point
  - -> VPC endpoint policy must enable connecting to access point & target bucket
- Object Lambda:
  - Use lambda function to change object before it is retrieved by caller app
  - Flow: caller app -> object lambda access point -> lambda function -> bucket access point -> bucket
  - Use cases:
    - Remove personal info before analyzing/using in non production env
    - Convert to dif data format (eg JSON -> XML)
    - Resize/watermark images