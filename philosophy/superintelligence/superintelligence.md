# Superintelligence

## Info
- Type: book
- Author: Nick Bostrom

## Category
- Philosophy
- Technology
- History

## Structure
- Part 1: possible pathways & timing
- Part 2: what happens after intelligence explosion
- Part 3: control problem
- Part 4: bigger picture

## Goals
- Problem: control general superintelligence
- -> Only have 1 chance since unfriendly superintelligence would try not to be controlled
- Provide a better view than existing views
- Create a basis for further discussion

## Style
- Ack by author:
  - Difficult subject, many uncertainty
  - -> Missing/wrong points could invalidate the conclusions
  - Focus more on risks instead of benefits: more urgent, existential issue
- Expository, casual, highly readable, with good sense of humor
- Highly futuristic but with good reasoning
- Useful introduction at the beginning of each chapter
- Ack of differing individual capability

## Recurring themes
- Accelerated development: once a higher level of intelligence is reached
- -> Convergence

## Terms
- Seed AI: early stage AI, not yet SI
- Computronium
- Wireheading (reinforcement learning): agent modifies its reward mechanism
- Desideratum: need
- Normative vs descriptive: ideal vs reality
- GI: p.4
- Intelligence explosion: p.5
- Evolution-based method: p.10
- AI complete problem: p.17
- AI vs software: p.19
- -> Single vs general purpose distinction
- Neuromorphic: p.34
- WBE: p.35
- Recursive self-improvement: p.35
- Biological cognition: p.43
- Intelligent vs wise: p.67
- Recalcitrance: p.79 (~return to scale)
- Overhang: p.89
- Decisive strategic advantage: p.96
- Wise-singleton sustainability threshold: p.124
- Orthogonal: p.129
- Instrumental goal: p.132
- Existential risk: p.140
- Malignant failure mode: causing existential catastrophes
- Perverse instantiation: p.146
- Infrastructure profusion: p.150
- Oracles: question-answering systems that have domain-general intelligence
- Associative value accretion for AI: specify a mechanism that leads to the acquisition of human values
when the AI interacts with a suitable env
- State risk vs step risk: p.287

## Criticism
- Lack of consideration regarding unexpected movement that slowdown/stop the progress: eg conflict, war
- Solving large scale problem (eg world control) is not easy, even for SI
- -> Might require lots of experiments over a long timescale
- There is an assumption that certain kinds of perverse instantiation are bad, but it is not necessarily true (eg maximizing pleasure)
- Chap 11 goes to far into the future, read like science fiction. The basic assumptions are not solid.
- Chap 11: arguments about population increase seems ungrounded

## Main content
### 1. Past developments and present capabilities
<img src="./resources/1.drawio.svg">

### 2. Paths to superintelligence
<img src="./resources/2.drawio.svg">

### 3. Forms of superintelligence
<img src="./resources/3.drawio.svg">

### 4. The kinetics of an intelligence explosion
<img src="./resources/4.drawio.svg">

### 5. Decisive strategic advantage
<img src="./resources/5.drawio.svg">

### 6. Cognitive superpowers
<img src="./resources/6.drawio.svg">

### 7. The superintelligence will
<img src="./resources/7.drawio.svg">

### 8. Is the default outcome doom?
<img src="./resources/8.drawio.svg">

### 9. The control problem
<img src="./resources/9.drawio.svg">

### 10. Oracles, genies, sovereigns, tools
<img src="./resources/10.drawio.svg">

### 11. Multipolar scenarios
<img src="./resources/11.drawio.svg">

### 12. Acquiring values
<img src="./resources/12.drawio.svg">

### 13. Choosing the criteria for choosing
- Problem: choose value to install without locking in prejudices & preconceptions of present generation
- <img src="./resources/13.drawio.svg">

## 14. The strategic picture
<img src="./resources/14.drawio.svg">

## 15. Crunch time
<img src="./resources/15.drawio.svg">

## Afterword
- Recent events & progress in AI
- Prediction
- Call for action